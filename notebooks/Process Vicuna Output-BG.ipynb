{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea3ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_id\tsentence_id\tdoc_start_index\tdoc_end_index\tsentence_entity_start\tsentence_entity_end\ttext_segment\tsemantic_type_id\tentity_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c13d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_checkpoint_mm_multi_sentences_671\n",
    "df_result_outputs = pd.read_csv('df_checkpoint_bg_test_sentences_prompt_0928_v1_13b.tsv', sep='\\t', header=0)\n",
    "df_result_outputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e15c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "df_result_outputs['outputs'] = df_result_outputs['outputs'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_outputs = list(df_result_outputs['outputs'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_outputs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561bcefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_occurences(occurences):\n",
    "    # intilize a null list\n",
    "    unique_list = []\n",
    "\n",
    "    # traverse for all elements\n",
    "    for x in occurences:\n",
    "        # check if exists in unique_list or not\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "\n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def find_original_span(text, term, abbr=False):    \n",
    "    found_original_text = ''\n",
    "    \n",
    "    search_text = text\n",
    "    search_term = term\n",
    "    if abbr and search_term in search_text:\n",
    "        found_original_text = search_term\n",
    "    else:       \n",
    "        search_text = text.lower()\n",
    "        search_term = term.lower()\n",
    "        if search_term in search_text:\n",
    "            found_original_text = search_term\n",
    "        #else:\n",
    "        #    found_original_text = fuzzy_ngram_match(term, text).lower()\n",
    "\n",
    "    if found_original_text != '':\n",
    "        print(found_original_text)\n",
    "        occurrences_start = [m.start() for m in re.finditer(re.escape(found_original_text), search_text)]\n",
    "        found_occurences = []\n",
    "        for start_idx in occurrences_start:\n",
    "            #start_idx = current_idx + text.lower()[current_idx:].index(found_original_text)\n",
    "            end_idx = start_idx + len(found_original_text)\n",
    "            found_occurences.append({\n",
    "                'start_idx': start_idx,\n",
    "                'end_idx': end_idx,\n",
    "                'term': text[start_idx:end_idx]\n",
    "            })\n",
    "        return found_occurences\n",
    "    else:\n",
    "        return [{\n",
    "            'start_idx': -1,\n",
    "            'end_idx': -1,\n",
    "            'term': 'no-match'\n",
    "        }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd612f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "def convert_output_from_comma_list(text, outputs, abbr=False):\n",
    "    outputs = str(outputs)\n",
    "    if ':' in outputs[0:150]:\n",
    "        outputs = outputs[outputs.index(':')+1:]\n",
    "    outputs_replaced = outputs.replace('*', '')\n",
    "    #.replace('\\\\n', ',')\n",
    "    term_lines = [term_line.strip() for term_line in outputs_replaced.split('\\n')]\n",
    "    terms_from_lines = [term_line.split(',') for term_line in term_lines]\n",
    "    terms = [term.strip() for term_line in terms_from_lines for term in term_line]\n",
    "    #print(f'terms: {terms}')\n",
    "    original_terms = []\n",
    "    for term in terms:\n",
    "        if len(term.split(' ')) < 10: # longer terms are probably explanations\n",
    "            original_term = find_original_span(text, term, abbr)\n",
    "            if len(original_term) > 0 and original_term[0]['start_idx'] > -1:\n",
    "                original_terms.extend(original_term)\n",
    "    \n",
    "    #todo: handle nested entities\n",
    "    return unique_occurences(original_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f765f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_translations(translation):\n",
    "    if len(translation) == 0 or translation == 'N/A' or translation.startswith('C0') or '\\\\' in translation:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d65245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "def convert_translation_from_comma_list(outputs):\n",
    "    outputs = str(outputs)\n",
    "    if ':\\n' in outputs[0:150]:\n",
    "        outputs = outputs[outputs.index(':')+1:]\n",
    "    outputs_replaced = outputs.replace('*', '')\n",
    "    #.replace('\\\\n', ',')\n",
    "    term_lines = [term_line.strip() for term_line in outputs_replaced.split('\\n')]\n",
    "    terms_from_lines = [term_line.split(',') for term_line in term_lines]\n",
    "    terms = [term.strip() for term_line in terms_from_lines for term in term_line]\n",
    "    #print(f'terms: {terms}')\n",
    "    original_terms = []\n",
    "    for term in terms:\n",
    "        if len(term.split(' ')) < 10: # longer terms are probably explanations\n",
    "            # handle term: translation\n",
    "            if term.startswith('1. ') or term.startswith('2. ') or term.startswith('3. ') or term.startswith('4. ') or term.startswith('5. ') or term.startswith('6. ') or term.startswith('7. ') or term.startswith('8. ') or term.startswith('9. '):                \n",
    "                term = term[3:]\n",
    "                # 1. - beginning number\n",
    "            \n",
    "            if ':' in term:\n",
    "                subterms = term.split(': ')\n",
    "                if len(subterms) == 2:\n",
    "                    original_terms.append(subterms[1])\n",
    "            elif '-' in term:\n",
    "                subterms = term.split(' - ') #\"оплаква се\" - complains\n",
    "                # term - C4440639\n",
    "                if len(subterms) == 2:\n",
    "                    if not subterms[1].startswith('C'):\n",
    "                        original_terms.append(subterms[1])\n",
    "                    else:\n",
    "                        original_terms.append(subterms[0])\n",
    "            elif '(' in term and ')' in term:\n",
    "                subterms = term.split('(')\n",
    "                for subterm in subterms:\n",
    "                    subterm = subterm.replace(')', '')\n",
    "                    if subterm.startswith('C') or subterm.startswith('S'):\n",
    "                        continue\n",
    "                    x = re.search(\"[а-яА-Я]*\", subterm)\n",
    "                    if len(x[0]) > 0:\n",
    "                        continue\n",
    "                # (C0694467) - replace suffix\n",
    "                #заболял (became ill)\n",
    "                #S0002556 (became ill)\n",
    "            else:\n",
    "                original_terms.append(term)\n",
    "    \n",
    "    #todo: handle nested entities\n",
    "    #return unique_occurences(original_terms)\n",
    "    return [x for x in original_terms if filter_translations(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc4fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences = pd.read_csv('../MedMentions-master/MedMentions-master/bg_texts_30.txt', sep='\\t', header=None)\n",
    "df_sentences.columns = ['sentence']\n",
    "df_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa393ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(df_sentences['sentence'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bb6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_original = []\n",
    "results_translated = []\n",
    "\n",
    "for text, outputs in zip(texts, result_outputs):    \n",
    "    terms = convert_output_from_comma_list(text, outputs[0])\n",
    "    print(terms)\n",
    "    result_original.append(terms)\n",
    "    #results_translated.append(outputs[1].split(','))\n",
    "    results_translated.append(convert_translation_from_comma_list(outputs[1]))\n",
    "    #combined = []\n",
    "    #for output in outputs:\n",
    "    #    combined += convert_output_from_comma_list(text, output)\n",
    "    #result_original.append(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e51aec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921cefe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcf46cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small_results = pd.DataFrame({'outputs': result_outputs, 'original': result_original, 'translated': results_translated}) #, 'medical_term': result_medical_terms})\n",
    "df_small_results.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d923b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df_sentences#[0:1221] #.tail(600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d25815",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outputs = df_small_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c426952",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outputs['sentence'] = list(df_subset['sentence'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6941ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55e1387",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outputs['entities'] = df_outputs['original']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edacdfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outputs.iloc[0]['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317d929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_entities = []\n",
    "\n",
    "for index, row in df_outputs.iterrows():\n",
    "    for entity, translation in zip(row['entities'], row['translated']):\n",
    "        if entity['start_idx'] > -1:\n",
    "            result_entities.append({\n",
    "                'document_id': index,\n",
    "                'sentence_id': 0,\n",
    "                'doc_start_index': 0,\n",
    "                'doc_end_index': len(row['sentence']),\n",
    "                'sentence_entity_start': entity['start_idx'],\n",
    "                'sentence_entity_end': entity['end_idx'],\n",
    "                'text_segment': entity['term'],\n",
    "                'translation': translation.lower()\n",
    "            })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190cf0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_entities = pd.DataFrame(result_entities)\n",
    "df_result_entities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef5f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_entities.to_csv('bg_test_0928v113b_llama_entities_nofuzzy.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f6c814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
